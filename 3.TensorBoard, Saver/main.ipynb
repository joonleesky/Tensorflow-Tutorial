{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hojoon/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/hojoon/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x):\n",
    "    X = np.zeros([len(x), max(x)+1])\n",
    "    X[np.arange(len(x)),x] = 1\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0,0], [1,0], [1,1], [0,0], [0,0], [0,1]])\n",
    "y_data = np.array([0,1,2,0,0,2])\n",
    "y_data = one_hot(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False, name = 'global_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer1\"):\n",
    "    W1 = tf.Variable(tf.random_uniform([2,5],-1.0, 1.0))\n",
    "    b1 = tf.Variable(tf.zeros([5]))\n",
    "    \n",
    "    h1 = tf.add(tf.matmul(X, W1),b1)\n",
    "    a1 = tf.nn.relu(h1)\n",
    "    \n",
    "    tf.summary.histogram(\"W1\",W1)\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random_uniform([5,10],-1.0, 1.0))\n",
    "    b2 = tf.Variable(tf.zeros([10]))\n",
    "    \n",
    "    h2 = tf.add(tf.matmul(a1, W2),b2)\n",
    "    a2 = tf.nn.relu(h2)\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"layer3\"):\n",
    "    W3 = tf.Variable(tf.random_uniform([10,3],-1.0, 1.0))\n",
    "    b3 = tf.Variable(tf.zeros([3]))\n",
    "    \n",
    "    score = tf.add(tf.matmul(a2, W3),b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = Y, logits = score))\n",
    "    \n",
    "# To track in the tensorboard\n",
    "tf.summary.scalar('loss',loss)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"optimizier\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-2)\n",
    "    train_op  = optimizer.minimize(loss, global_step= global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"prediction\"):\n",
    "    pred   = tf.argmax(score, 1)\n",
    "    target = tf.argmax(Y, 1)\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.to_float(tf.equal(pred, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:1   Loss:1.593\n",
      "Step:2   Loss:1.492\n",
      "Step:3   Loss:1.404\n",
      "Step:4   Loss:1.328\n",
      "Step:5   Loss:1.260\n",
      "Step:6   Loss:1.198\n",
      "Step:7   Loss:1.143\n",
      "Step:8   Loss:1.095\n",
      "Step:9   Loss:1.056\n",
      "Step:10   Loss:1.021\n",
      "Step:11   Loss:0.988\n",
      "Step:12   Loss:0.960\n",
      "Step:13   Loss:0.935\n",
      "Step:14   Loss:0.910\n",
      "Step:15   Loss:0.886\n",
      "Step:16   Loss:0.863\n",
      "Step:17   Loss:0.841\n",
      "Step:18   Loss:0.820\n",
      "Step:19   Loss:0.800\n",
      "Step:20   Loss:0.781\n",
      "Step:21   Loss:0.763\n",
      "Step:22   Loss:0.745\n",
      "Step:23   Loss:0.731\n",
      "Step:24   Loss:0.716\n",
      "Step:25   Loss:0.701\n",
      "Step:26   Loss:0.686\n",
      "Step:27   Loss:0.671\n",
      "Step:28   Loss:0.655\n",
      "Step:29   Loss:0.639\n",
      "Step:30   Loss:0.624\n",
      "Step:31   Loss:0.607\n",
      "Step:32   Loss:0.593\n",
      "Step:33   Loss:0.577\n",
      "Step:34   Loss:0.562\n",
      "Step:35   Loss:0.548\n",
      "Step:36   Loss:0.534\n",
      "Step:37   Loss:0.521\n",
      "Step:38   Loss:0.508\n",
      "Step:39   Loss:0.494\n",
      "Step:40   Loss:0.482\n",
      "Step:41   Loss:0.469\n",
      "Step:42   Loss:0.455\n",
      "Step:43   Loss:0.443\n",
      "Step:44   Loss:0.431\n",
      "Step:45   Loss:0.419\n",
      "Step:46   Loss:0.407\n",
      "Step:47   Loss:0.396\n",
      "Step:48   Loss:0.385\n",
      "Step:49   Loss:0.374\n",
      "Step:50   Loss:0.364\n",
      "Step:51   Loss:0.354\n",
      "Step:52   Loss:0.344\n",
      "Step:53   Loss:0.334\n",
      "Step:54   Loss:0.325\n",
      "Step:55   Loss:0.316\n",
      "Step:56   Loss:0.307\n",
      "Step:57   Loss:0.298\n",
      "Step:58   Loss:0.290\n",
      "Step:59   Loss:0.281\n",
      "Step:60   Loss:0.273\n",
      "Step:61   Loss:0.266\n",
      "Step:62   Loss:0.258\n",
      "Step:63   Loss:0.251\n",
      "Step:64   Loss:0.244\n",
      "Step:65   Loss:0.237\n",
      "Step:66   Loss:0.230\n",
      "Step:67   Loss:0.223\n",
      "Step:68   Loss:0.216\n",
      "Step:69   Loss:0.210\n",
      "Step:70   Loss:0.204\n",
      "Step:71   Loss:0.198\n",
      "Step:72   Loss:0.192\n",
      "Step:73   Loss:0.186\n",
      "Step:74   Loss:0.180\n",
      "Step:75   Loss:0.175\n",
      "Step:76   Loss:0.169\n",
      "Step:77   Loss:0.164\n",
      "Step:78   Loss:0.158\n",
      "Step:79   Loss:0.153\n",
      "Step:80   Loss:0.149\n",
      "Step:81   Loss:0.144\n",
      "Step:82   Loss:0.139\n",
      "Step:83   Loss:0.135\n",
      "Step:84   Loss:0.130\n",
      "Step:85   Loss:0.126\n",
      "Step:86   Loss:0.121\n",
      "Step:87   Loss:0.117\n",
      "Step:88   Loss:0.113\n",
      "Step:89   Loss:0.110\n",
      "Step:90   Loss:0.106\n",
      "Step:91   Loss:0.102\n",
      "Step:92   Loss:0.099\n",
      "Step:93   Loss:0.095\n",
      "Step:94   Loss:0.092\n",
      "Step:95   Loss:0.089\n",
      "Step:96   Loss:0.086\n",
      "Step:97   Loss:0.083\n",
      "Step:98   Loss:0.080\n",
      "Step:99   Loss:0.077\n",
      "Step:100   Loss:0.075\n",
      "\n",
      "Prediction: [0 1 2 0 0 2]\n",
      "True:       [0 1 2 0 0 2]\n",
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Store all the variables into saver\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    \n",
    "    # Check Whether there exits a saved model\n",
    "    ckpt  = tf.train.get_checkpoint_state('./model')\n",
    "    # Restore the variables if there exists a saved model \n",
    "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Allocate the directory to save the graph\n",
    "    writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "    \n",
    "    # Train\n",
    "    for step in range(100):\n",
    "        sess.run(train_op, feed_dict = {X:x_data, Y:y_data})\n",
    "        \n",
    "        print('Step:%d  ' % sess.run(global_step),\n",
    "              'Loss:%.3f' % sess.run(loss, feed_dict = {X:x_data, Y:y_data}))\n",
    "    \n",
    "        # Save the summary\n",
    "        summary = sess.run(merged, feed_dict = {X:x_data, Y:y_data})\n",
    "        writer.add_summary(summary, global_step = sess.run(global_step))\n",
    "    writer.close()\n",
    "    \n",
    "    # Save the model\n",
    "    saver.save(sess, './model/dnn.ckpt', global_step = global_step)\n",
    "    \n",
    "    \n",
    "    # Prediction\n",
    "    print('\\nPrediction:', sess.run(pred, feed_dict = {X:x_data, Y:y_data}))\n",
    "    print('True:      ', np.argmax(y_data,1))\n",
    "    print('Accuracy: %.2f' %sess.run(accuracy, feed_dict = {X:x_data, Y:y_data}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
